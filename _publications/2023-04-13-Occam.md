---
title: "Do deep neural networks have an inbuilt Occam's razor?"
collection: preprints
permalink: /publications/2023-Occam
excerpt: 'Automatic gradient descent trains both fully-connected and convolutional networks out-of-the-box and at ImageNet scale without hyperparameters'
date: 2023-04-13
venue: 'arxiv'
citation: 'Jeremy Bernstein, Chris Mingard, Kevin Huang, Navid Azizan, Yisong Yue. "Automatic Gradient Descent: Deep Learning without Hyperparameters." arXiv preprint 	arXiv:2304.05187 (2023).'
---

The remarkable performance of overparameterized deep neural networks (DNNs) must arise from an interplay between network 
architecture, training algorithms, and structure in the data. To disentangle these three components, we apply a Bayesian 
picture, based on the functions expressed by a DNN, to supervised learning. The prior over functions is determined by the 
network, and is varied by exploiting a transition between ordered and chaotic regimes. For Boolean function classification, 
we approximate the likelihood using the error spectrum of functions on data. When combined with the prior, this accurately 
predicts the posterior, measured for DNNs trained with stochastic gradient descent. This analysis reveals that structured 
data, combined with an intrinsic Occam's razor-like inductive bias towards (Kolmogorov) simple functions that is strong 
enough to counteract the exponential growth of the number of functions with complexity, is a key to the success of DNNs.


[Download paper here](http://c1510.github.io/files/Occam.pdf)
